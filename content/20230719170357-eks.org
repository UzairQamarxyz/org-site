:PROPERTIES:
:ID:       7953663e-8a6b-4757-985f-cfd890e3315c
:END:
#+title: EKS
#+description: Notes on EKS
#+author: Uzair Qamar
#+setupfile: rethink_inline.theme
#+options: num:nil

* Introduction
AWS EKS (Elastic Kubernetes Service) is an AWS managed service to run Kubernetes in the AWS cloud.

** Control Plane
Think of it like the brain of the cluster. It's responsible for storing the state of all the components. It is also responsible for assigning the pods to suitable nodes and saving their metadata to ensure consistency.
Control Plane is a general term here. This plane actually contains other components as well like ETCD, API Server.
The control plane exists inside an AWS managed VPC. It automatically scales based on the data plane; patching etc.
** Data Plane
This contains the workers for the kubernetes cluster. All the pods run in this plane on computes called "nodes" and the entire plane itself runs inside your VPC.
There are several ways to run nodes for your dataplane:
1. *Self-Managed*: You manage the nodes inside the cluster yourself (provisioning, registering etc.)
2. *Managed*: AWS manages the provisioning and lifecycle management of the nodes.
3. *Fargate*: AWS provides on-demand, right-sized compute for your containers. No need to provision, configure or scale.

* Cluster Networking
TODO: Add cluster Networking
https://aws.amazon.com/blogs/containers/de-mystifying-cluster-networking-for-amazon-eks-worker-nodes/

* Authentication & Authorization
AWS EKS uses IAM to provide authentication to your clusters. AWS IAM only handles the authentication to the cluster, authorization is handled by RBAC inside the cluster.

#+ATTR_ORG: :width 1200
[[file:media/EKS/authentication_authorization.png][Authentication & Authorization]]

** Authentication
There are different ways to authenticate a request to the kube-apiserver service, e.g. Bearer Tokens, X.509 Certificates, OIDC, etc. EKS has native support for [[https://kubernetes.io/docs/reference/access-authn-authz/authentication/#webhook-token-authentication][webhook tokens & OIDC]] at the time of writing.
Webhook authentication strategy works by passing bearer tokens to a webhook that verifies the bearer tokens. These bearer tokens are generated by the AWS CLI. As you run ~kubectl~ commands, the token is passed to the kube-apiserver which forwards it to an [[https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#what-are-admission-webhooks][admission webhook]]. If the request is well-formed, the webhook calls a pre-signed URL embedded in the token. This URL verifies the request's signature and returns information about the user to the kube-apiserver.
To get a token from the command line, run the following command.
#+begin_src shell
aws eks get-token --cluster <cluster-name>
#+end_src

Here's how a token looks like.
#+begin_src json
{
  "kind": "ExecCredential",
  "apiVersion": "client.authentication.k8s.io/v1alpha1",
  "spec": {},
  "status": {
    "expirationTimestamp": "2020-02-19T16:08:27Z",
    "token": "k8s-aws-v1.aHR0cHM6Ly9zdHMuYW1hem9uYXdzLmNvbS8_QWN0aW9uPUdldENhbGxlcklkZW50aXR5JlZlcnNpb249MjAxMS0wNi0xNSZYLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFKTkdSSUxLTlNSQzJXNVFBJTJGMjAyMDAyMTklMkZ1cy1lYXN0LTElMkZzdHMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDIwMDIxOVQxNTU0MjdaJlgtQW16LUV4cGlyZXM9NjAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JTNCeC1rOHMtYXdzLWlkJlgtQW16LVNpZ25hdHVyZT0yMjBmOGYzNTg1ZTMyMGRkYjVlNjgzYTVjOWE0MDUzMDFhZDc2NTQ2ZjI0ZjI4MTExZmRhZDA5Y2Y2NDhhMzkz"
  }
}
#+end_src

Each token starts with ~k8s-aws-v1.~ followed by a base64 encoded string. When decoded, it should look like the following.
#+begin_quote
https://sts.amazonaws.com/?Action=GetCallerIdentity&Version=2011-06-15&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJPFRILKNSRC2W5QA%2F20200219%2Fus-east-1%2Fsts%2Faws4_request&X-Amz-Date=20200219T155427Z&X-Amz-Expires=60&X-Amz-SignedHeaders=host%3Bx-k8s-aws-id&X-Amz-Signature=220f8f3285e320ddb5e683a5c9a405301ad76546f24f28111fdad09cf648a393
#+end_quote

** Authorization
Once the user identity has been confirmed by the AWS IAM service, the kube-serverapi reads the ~aws-auth~ configmap in the ~kube-system~ namespace. The ~aws-auth~ configmap provides a static mapping between the IAM principals, i.e. IAM Roles and Users, and the Kubernetes RBAC groups. RBAC groups can be referenced in Kubernetes RoleBindings or ClusterRoleBindings. They are similar to IAM Roles in that they define a set of actions (verbs) that can be performed against a collection of Kubernetes resources (objects).
By default the IAM Principal that provisioned the cluster is granted the ~system:masters~ permissions. This access can not be removed and is not managed by the ~aws-auth~ configmap. It is, therefore, recommended to use a *dedicated IAM role* for the creation of the EKS clusters.
In order to grant additional IAM Principals access to the cluster, the ~aws-auth~ config-map needs to be edited with the appropriate entry.
#+begin_src yaml
kubectl get cm aws-auth -n kube-system -oyaml

kind: ConfigMap
apiVersion: v1
metadata:
  name: aws-auth
  namespace: kube-system
data:
  mapRoles: |
    # Exists by default
    - groups:
      - system:bootstrappers
      - system:nodes
      rolearn: arn:aws:iam::XXXXXXXXXXXX:role/EKSWorkerNodesRole
      username: system:node:{{EC2PrivateDNSName}}
    # Added for fargate compatibility
    - groups:
      - system:bootstrappers
      - system:nodes
      - system:node-proxier
      rolearn: arn:aws:iam::XXXXXXXXXXXX:role/EKSFargateExecutionRole
      username: system:node:{{SessionName}}
    # Added for Karpenter compatibility
    - groups:
      - system:bootstrappers
      - system:nodes
      rolearn: arn:aws:iam::XXXXXXXXXXXX:role/KarpenterNodeRole
      username: system:node:{{EC2PrivateDNSName}}
#+end_src

* Addons
** VPC CNI Plugin
#+ATTR_ORG: :width 1200
[[https://aws.github.io/aws-eks-best-practices/networking/vpc-cni/image.png][VPC CNI]]
It's a Container Network Interface for your EKS cluster. It allows the pods in your cluster to be assigned IPs from the VPC directly via the ENI attached to the nodes. Since each ENI can have a limited amount of IPs assigned to it and each node can have a limited amount of ENIs attached, this limits the amount of pods you can run on a node. Check[[https://github.com/awslabs/amazon-eks-ami/blob/master/files/eni-max-pods.txt][ here]] to see how many pods can be allocated for each node type.
This plugin creates a *primary ENI* and assigns a primary IPs to the ENI. These primary IPs are used by the node itself and the pods running on it in hostNetwork mode. When the maximum number of IPs are reached for an ENI, the plugin creates another ENI and attaches to the node. This is done until the node runs out of resources to spare for further ENIs.
In order to speed up the networking process, some "warm" ENIs may be ready in advance.
*** Configuration
The number of ENIs and the IP addresses in a pool are configured through the ~WARM_ENI_TARGET~, ~WARM_IP_TARGET~ and ~MINIMUM_IP_TARGET~.
+ *WARM_ENI_TARGET*: The number of secondary ENIs attached to the node on standby. An ENI is considered "warm" when its IP isn't associated with a pod. After the IP is associated, another ENI is created and attached as a "warm" ENI in order to maintain the targeted warm ENIs.
+ *WARM_IP_TARGET*: The number of IPs that may be available without requiring an additional ENI.
+ *MINIMUM_IP_TARGET*: The minimum number of IP addresses to be allocated at any time.
*** Prefix Assignment
#+ATTR_ORG: :width 1200
[[https://aws.github.io/aws-eks-best-practices/networking/prefix-mode/image.png][Prefix Assignment Mode]]
[[https://docs.aws.amazon.com/eks/latest/userguide/cni-increase-ip-addresses.html][Prefix assignment mode]] enables us to run more pods on *AWS Nitro* based instance types. Now instead of assigning individual IPs to the ENI, the ENI gets assigned /28 IP prefixes which increases the IP pool. This allows us to have a larger IP pool while the number of ENIs stay the same.
[[https://aws.github.io/aws-eks-best-practices/networking/prefix-mode/image-2.jpeg][Prefix Assignment Flow]]


** CoreDNS
CoreDNS is a flexible and extensible DNS server that can serve for the Kubernetes cluster DNS. It is installed by default. It provides name resolution for all the pods in your cluster.

** Kube-Proxy
Maintains network rules on each Amazon EC2 node. It enables network communication to your Pods. The self-managed or managed type of this add-on is installed on each Amazon EC2 node in your cluster, by default.
